---
title: "Project"
author: "Travis Benedict"
date: "October 30, 2018"
output: pdf_document
---
```{r echo=TRUE}
library(magrittr)
library(dplyr)
library(mclust)
library(VGAM)
library(mapproj)
library(ggmap)
library(ggplot2)
library(maps)
library(mapdata)
library(choroplethr)
library(choroplethrMaps)


#Packages you made to Install
# install.packages(c("choroplethr", "choroplethrMaps")) 
```


#Format Data
```{r echo=TRUE}
View(results)
View(facts)
View(joined_facts_results)
View(descriptions)

# Predictors are separated from vote results join on FIPS
results = read.csv('CountyLevelResults.csv')
facts = read.csv('county_facts.csv')
descriptions = read.csv('county_facts_dictionary.csv')

###Point Diff Column Calculation
results$points_diff_rep = ((results$votes_dem - results$votes_gop)/results$total_votes)*-1


###Lean Calculation
results$Lean <- cut(results$points_diff_rep, 
                       breaks = c(-1, -.34, -.12, -.07, -.04, .04, .07, .12, .34, 1), 
                       labels = c("Solid Democratic", "Likely Democratic", "Lean Democratic", "Tilt Democratic", "Toss-Up", "Tilt GOP", "Lean GOP", "Likely GOP", "Solid GOP"), 
                       right = FALSE)
results$Lean_Number <- as.numeric(results$Lean)


###Winner Calculation
results$Winner <- cut(results$points_diff_rep, 
                       breaks = c(-1, 0, 1), 
                       labels = c("Democratic", "Republican"), 
                       right = FALSE)
results$Winner_Number <- as.numeric(results$Winner)

# Remove rows without results and identifier columns

joined_facts_results = merge(facts, results[c("combined_fips", "Lean", "Lean_Number", "points_diff_rep", "Winner", "Winner_Number")], by.x="Fips", by.y="combined_fips")
head(joined_facts_results)

cleaned_facts = data.frame(joined_facts_results[c(-1, -2, -3, -length(joined_facts_results[1,]), -(length(joined_facts_results[1,])-1), -(length(joined_facts_results[1,])-2), -(length(joined_facts_results[1,])-3), -(length(joined_facts_results[1,])-4), -(length(joined_facts_results[1,])-5), -(length(joined_facts_results[1,])-6))])

scaled_facts = scale(cleaned_facts)
```


#Baseline Logit Model
```{r echo=TRUE}
set.seed(1)

# Baseline category logit model without variable selection on entire dataset
samp <- sample(1:3100, 250)
logit <- vglm(joined_facts_results$Lean[samp] ~ ., data=data.frame(scaled_facts[samp,]), 
              family=(multinomial))

#summary(logit)


plot(hclust(dist(scaled_facts[sample(length(scaled_facts[,1]), 100),]), method="complete"), main="Complete Linkage", labels=F)
plot(hclust(dist(scaled_facts[sample(length(scaled_facts[,1]), 100),]), method="single"), main="Single Linkage",  labels=F)
plot(hclust(dist(scaled_facts[sample(length(scaled_facts[,1]), 100),]), method="average"), main="Average Linkage",  labels=F)
```


#Principal Component Analysis and Clustering
```{r echo=TRUE}
#########PCA
prin_comp <- prcomp(scaled_facts, scale. = F)

summary(prin_comp)

#compute standard deviation of each principal component
std_dev <- prin_comp$sdev

#compute variance
pr_var <- std_dev^2

#check variance of first 10 components
pr_var[1:length(scaled_facts)]

#proportion of variance explained
prop_varex <- pr_var/sum(pr_var)
prop_varex[1:length(scaled_facts)]

#scree plot
plot(prop_varex, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")

#cumulative scree plot
plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")
abline(h=0.9)
abline(v=17)


# COMBINE THE FACTS WITH the LABEL COLUMN USING FIPS
autoplot(prin_comp, data = scaled_facts, colour = 'Lean', loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3) +
  ggtitle("First two PC's using Original Variables")


# Data from first 17 principal components
pc_data <- prin_comp$x[,1:17]

plot(hclust(dist(pc_data[sample(length(pc_data[,1]), 250),]), method="complete"), main="Complete Linkage", labels=F)



c_pc = mclustBIC(pc_data, G=1:20)
plot(c_pc)

# Best cluster model is k=10 VVV
summary(c_pc)

em_clusters_pc = Mclust(pc_data, G=10, modelNames = "VVV")

# Possibly compare the EM results to kmeans with k=10

cluster_id_pc = em_clusters_pc$classification

cluster_count_pc = data.frame(cluster_id_pc) %>%
  group_by(cluster_id_pc) %>%
  summarise(n = n())

state_cluster_counts_pc = cbind(joined_facts_results[1:3], cluster_id_pc) %>% 
  group_by(state_abbreviation, cluster_id_pc) %>%
  summarise(count = n())

Lean_cluster_counts_pc = data.frame(cbind(joined_facts_results[length(joined_facts_results)], cluster_id_pc)) %>% 
  group_by(Lean, cluster_id_pc) %>%
  summarise(count = n())

View(Lean_cluster_counts_pc)

plot(jitter(cluster_id_pc), jitter(as.numeric(joined_facts_results$Lean)))

Lean_counts = joined_facts_results[length(joined_facts_results)] %>%
  group_by(Lean) %>%
  summarise(count = n())

merged_lean_counts_pc = merge(Lean_cluster_counts_pc, Lean_counts, by.x="Lean", by.y="Lean")
merged_lean_counts_pc$pct = merged_lean_counts_pc$count.x / merged_lean_counts_pc$count.y

plot(merged_lean_counts_pc$cluster_id, as.numeric(merged_lean_counts_pc$Lean), cex = 10 * merged_lean_counts_pc$pct, xlab = "Cluster ID", ylab = "Solid Dem to Solid GOP", main = "Percentage of Lean By Cluster Based on First 17 Pcs")


# Try clustering with scaled data instead of pc
c = mclustBIC(scaled_facts, G=5:15)
plot(c)

# Best cluster model is k=7 VEV
summary(c)

em_clusters = Mclust(scaled_facts, G=7, modelNames = "VEV")

# Possibly compare the EM results to kmeans with k=10

cluster_id = em_clusters$classification

cluster_count = data.frame(cluster_id) %>%
  group_by(cluster_id) %>%
  summarise(n = n())

state_cluster_counts = cbind(joined_facts_results[1:3], cluster_id) %>% 
  group_by(state_abbreviation, cluster_id) %>%
  summarise(count = n())

Lean_cluster_counts = data.frame(cbind(joined_facts_results[length(joined_facts_results)], cluster_id)) %>% 
  group_by(Lean, cluster_id) %>%
  summarise(count = n())

View(Lean_cluster_counts)

# Note that there are many more Solid GOP than Solid Dem counties
plot(jitter(cluster_id), jitter(as.numeric(joined_facts_results$Lean)))

merged_lean_counts = merge(Lean_cluster_counts, Lean_counts, by.x="Lean", by.y="Lean")
merged_lean_counts$pct = merged_lean_counts$count.x / merged_lean_counts$count.y

plot(merged_lean_counts$cluster_id, as.numeric(merged_lean_counts$Lean), cex = 10 * merged_lean_counts$pct, xlab = "Cluster ID", ylab = "Solid Dem to Solid GOP", main = "Percentage of Lean By Cluster")



# Try clustering just based on pct non hispanic White, pct college degree, per capita income
expected_trump = scaled_facts[, c("RHI825214", "EDU685213", "INC910213")] 

trump_bic = mclustBIC(expected_trump, G=1:20)
plot(trump_bic)

# Best cluster model is k=8 VVV
summary(trump_bic)

em_trump_clusts = Mclust(expected_trump, G=8, modelNames = "VVV")

cluster_id_trump = em_trump_clusts$classification

cluster_count_trump = data.frame(cluster_id_trump) %>%
  group_by(cluster_id_trump) %>%
  summarise(n = n())

state_cluster_counts_trump = cbind(joined_facts_results[1:3], cluster_id_trump) %>% 
  group_by(state_abbreviation, cluster_id_trump) %>%
  summarise(count = n())

Lean_cluster_counts_trump = data.frame(cbind(joined_facts_results[length(joined_facts_results)], cluster_id_trump)) %>% 
  group_by(Lean, cluster_id_trump) %>%
  summarise(count = n())

merged_lean_counts_trump = merge(Lean_cluster_counts_trump, Lean_counts, by.x="Lean", by.y="Lean")
merged_lean_counts_trump$pct = merged_lean_counts_trump$count.x / merged_lean_counts_trump$count.y

plot(merged_lean_counts_trump$cluster_id, as.numeric(merged_lean_counts_trump$Lean), cex = 10 * merged_lean_counts_trump$pct, xlab = "Cluster ID", ylab = "Solid Dem to Solid GOP", main = "% of Lean By Cluster Based on % White, % College Degree, Income")

clPairs(expected_trump, joined_facts_results[,length(joined_facts_results)], lower.panel=NULL)
clPairsLegend(0, 0.46, class = clp$class,
col = clp$col, pch = clp$pch)
```


#Maps
```{r echo=TRUE}
###Political Lean by County (State and County Outline)
colors = c("#0000ff", "#2020df", "#4040bf", "#60609f", "#808080", "#9f6060", "#bf4040", "#df2020", "#ff0000")
colorsmatched <- joined_facts_results$Lean_Number[match(county.fips$fips, joined_facts_results$Fips)]

map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0, 
    lty = 0, projection = "polyconic")
# Add border around each State
map("county", col = "white", fill = FALSE, add = TRUE, lty = 1, lwd = .3, 
    projection = "polyconic")
map("state", col = "white", fill = FALSE, add = TRUE, lty = 1, lwd = .8, 
    projection = "polyconic")
title("Political Lean")
leg.txt <- c("Solid Democratic", "Likely Democratic", "Lean Democratic", "Tilt Democratic", "Toss-Up", "Tilt GOP", "Lean GOP", "Likely GOP", "Solid GOP")
legend("top", leg.txt, horiz = TRUE, fill = colors, cex=.35)


###Winner by County (State and County Outline)
colors = c("#0000ff", "#ff0000")
colorsmatched <- joined_facts_results$Winner_Number[match(county.fips$fips, joined_facts_results$Fips)+1]

map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0, 
    lty = 0, projection = "polyconic")
# Add border around each State
map("county", col = "white", fill = FALSE, add = TRUE, lty = 1, lwd = .3, 
    projection = "polyconic")
map("state", col = "white", fill = FALSE, add = TRUE, lty = 1, lwd = .8, 
    projection = "polyconic")
title("Political Lean")
leg.txt <- c("Democratic", "Republican")
legend("top", leg.txt, horiz = TRUE, fill = colors, cex=.5)


#Demographic Maps

#Percent Non-Hispanic White
FIPS_White <- joined_facts_results %>% select(region=Fips, value=RHI825214)
county_choropleth(FIPS_White,
                  title ="2016 County Percent Non-Hispanic White", 
                 legend = "Percent Non-Hispanic White")

#Percent College Degree
FIPS_College_Degree <- joined_facts_results %>% select(region=Fips, value=EDU685213)
county_choropleth(FIPS_College_Degree,
                  title ="2016 County Percent College Degree", 
                 legend = "Percent College Degree")

#Per Capita Income
FIPS_PCI <- joined_facts_results %>% select(region=Fips, value=INC910213)
county_choropleth(FIPS_PCI,
                  title ="2016 County Per Capita Income", 
                 legend = "Per Capita Income")


###Political Lean by County (State and County Outline)
# usa <- map_data("usa")
# states <- map_data("state")
# county <- map_data("county")
# 
# 
# ggplot(data = county) + 
#   geom_polygon(aes(x = long, y = lat, fill = region, group = group), color = "white") + 
#   coord_fixed(1.3) +
#   guides(fill=FALSE)  # do this to leave off the color legend
# 
# 
# 
# 
# 
# 
# 
# 
# map("county", fill = TRUE, resolution = 0, 
#     lty = 0, projection = "polyconic") + scale_fill_gradient(trans = "log10")
# 
# 
# 
# # counties <- map_data("county")
# # USA_counties <- ggplot(data = counties, mapping = aes(x = long, y = lat, group = group)) + 
# #   coord_fixed(1.3) + 
# #   geom_polygon(color = "black", fill = "gray")
# # 
# # USA_counties + geom_polygon(data = cleaned_facts, aes(fill = RHI825214), color = "white")
# # 
# # cleaned_facts %>%
# #   ggplot(aes(long, lat, group = group, fill = RHI825214)) +
# #   geom_polygon(color = NA) +
# #   coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
# #   labs(fill = "Median Household Income")
# # 
# # 
# # cleaned_facts %>%
# #   ggplot(aes(long, lat, group = group, fill = RHI825214)) +
# #   geom_polygon(color = NA) +
# #   scale_fill_gradientn(labels = scales::percent,
# #                        guide = guide_colorbar(title.position = "top")) +
# #   geom_polygon(data = states, mapping = aes(long, lat, group = group),
# #                fill = NA, color = "#ffffff") +
# #   coord_map(projection = "albers", lat0 = 39, lat1 = 45) +
# #   theme(legend.title = element_text(),
# #         legend.key.width = unit(.5, "in")) +
# #   labs(fill = "Homeownership rate") +
# #   theme_urban_map()
# # 
# # df1 <- joined_facts_results %>% select(subregion=County, region=State, B20004001 )
# # 
# # 
# # ggplot(cleaned_facts, aes(long, lat,group = group)) + 
# #   geom_polygon(aes(fill = RHI825214), colour = rgb(1,1,1,0.2))  +
# #   coord_quickmap()

```



# Model Fitting
```{r, echo=TRUE}
library(VGAM)
set.seed(1)

# Fit a logit model with just non hispanic white and college dregree
joined_facts_results$GOPDem[joined_facts_results$points_diff_rep < 0] <- 0
joined_facts_results$GOPDem[joined_facts_results$points_diff_rep > 0] <- 1

kFold <- function(x, k) split(sample(1:nrow(x)), cut(seq_along(sample(1:nrow(x))), k, labels = FALSE)) 

kFoldCVLogit <- function(y, x, k, columns, link){
  folds = kFold(x, k)

  mses = c()
  for (i in 1:k) {
    testIndexes = folds[[i]]
    testData = x[testIndexes, ]
    trainData = x[-testIndexes, ]
    
    fit <- glm(y[-testIndexes] ~ ., data=data.frame(trainData[,columns]), 
                family=(binomial(link = link)))
    predicted <- predict(fit, data.frame(testData[,columns]), type="response")
    mse = sum((y[testIndexes] - predicted)^2)
    mses = c(mses, mse)
  }

list(mse, fit)
}
y = joined_facts_results$GOPDem
x = scaled_facts
k = 10
columns = c("RHI825214", "EDU685213")
link = "logit"
result = kFoldCVLogit(y, x, k, columns, link)
mse = result[[1]]
model = result[[2]]



# MAKE NOTE OF SCORING USED FOR CROSS VALIDATION

kFoldCVMultinomial <- function(y, x, k, columns, link){
  folds = kFold(x, k)

  mses = c()
  for (i in 1:k) {
    testIndexes = folds[[i]]
    testData = x[testIndexes, ]
    trainData = x[-testIndexes, ]
    
    fit <- vglm(y[-testIndexes] ~ ., data=data.frame(trainData[,columns]), 
                family=(multinomial))
    predicted <- predict(fit, data.frame(testData[,columns]), type="response")
    # Calculating the MSE using Brier Score
    mse = sum((y[testIndexes] - predicted[cbind(seq_along(y[testIndexes]), y[testIndexes])])^2) 
    
    mses = c(mses, mse)
  }
  
list(sum(mses) / nrow(x), fit)
}
y = as.numeric(joined_facts_results$Lean)
x = scaled_facts
k = 10
columns = c("SBO315207", "EDU685213")
link = "logit"
result = kFoldCVMultinomial(y, x, k, columns, link)
mse = result[[1]]
model = result[[2]]
summary(model)

y = as.numeric(joined_facts_results$Lean)
x = scaled_facts
k = 10
columns = c("RHI825214", "EDU685213")
link = "logit"

samp <- sample(1:3100, 250)
logit <- vglm(joined_facts_results$Lean[samp] ~ ., data=data.frame(scaled_facts[samp,c("RHI825214", "EDU685213")]), 
              family=(multinomial))


# Perform forward and backward AIC/BIC variable selection
library(glmnet)

y = joined_facts_results$GOPDem
x = scaled_facts
link = "logit"
fit_full <- glm(y[-testIndexes] ~ ., data=data.frame(x[-testIndexes,]), 
                family=(binomial(link = link)))
fit_null <- glm(y[-testIndexes] ~ 1, data=data.frame(x[-testIndexes,]), family = binomial(link = "logit"))

# Forward AIC:
forAIC = step(fit_null,  # Start with NULL model
               scope = list(lower=fit_null, upper=fit_full), # Range of models
               direction="forward",
              trace = FALSE) # Forward or backward

# Backward AIC:
backAIC = step(fit_full,  # Start with NULL model
               scope = list(lower=fit_null, upper=fit_full), # Range of models
               direction="backward",
              trace = FALSE) # Forward or backward
summary(backAIC)

# Forward BIC:
forBIC = step(fit_null,  # Start with NULL model
               scope = list(lower=fit_null, upper=fit_full), # Range of models
               direction="forward",
              k = log(length(y)),
              trace = FALSE) # Forward or backward

# Forward BIC:
backBIC = step(fit_full,  # Start with NULL model
               scope = list(lower=fit_null, upper=fit_full), # Range of models
               direction="backward",
              k = log(length(y)),
              trace = FALSE) # Forward or backward
summary(backBIC)

# Forward Backward AIC/BIC for probit
link = "probit"
fit_full <- glm(y[-testIndexes] ~ ., data=data.frame(x[-testIndexes,]), 
                family=(binomial(link = link)))
fit_null <- glm(y[-testIndexes] ~ 1, data=data.frame(x[-testIndexes,]), family = binomial(link = "logit"))

# Forward AIC:
forAIC = step(fit_null,  # Start with NULL model
               scope = list(lower=fit_null, upper=fit_full), # Range of models
               direction="forward",
              trace = FALSE) # Forward or backward

# Backward AIC:
backAIC = step(fit_full,  # Start with NULL model
               scope = list(lower=fit_null, upper=fit_full), # Range of models
               direction="backward",
              trace = FALSE) # Forward or backward
summary(backAIC)

# Forward BIC:
forBIC = step(fit_null,  # Start with NULL model
               scope = list(lower=fit_null, upper=fit_full), # Range of models
               direction="forward",
              k = log(length(y)),
              trace = FALSE) # Forward or backward

# Forward BIC:
backBIC = step(fit_full,  # Start with NULL model
               scope = list(lower=fit_null, upper=fit_full), # Range of models
               direction="backward",
              k = log(length(y)),
              trace = FALSE) # Forward or backward
summary(backBIC)


# Take each of these models and perform 10 Fold CV compare to our Baseline model of White and Education

```
