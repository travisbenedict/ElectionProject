---
title: "Project"
author: "Travis Benedict"
date: "October 30, 2018"
output: pdf_document
---

```{r echo=TRUE}
set.seed(1)

View(results)

# Predictors are separated from vote results join on FIPS
results = read.csv('CountyLevelResults.csv')
facts = read.csv('county_facts.csv')
descriptions = read.csv('county_facts_dictionary.csv')

###Point Diff Column Calculation
results$points_diff_rep = ((results$votes_dem - results$votes_gop)/results$total_votes)*-1


results$Lean <- cut(results$points_diff_rep, 
                       breaks = c(-1, -.34, -.12, -.07, -.04, .04, .07, .12, .34, 1), 
                       labels = c("Solid Democratic", "Likely Democratic", "Lean Democratic", "Tilt Democratic", "Toss-Up", "Tilt GOP", "Lean GOP", "Likely GOP", "Solid GOP"), 
                       right = FALSE)

# Remove rows without results and identifier columns
joined_facts_results = merge(facts, results[c("combined_fips", "Lean")], by.x="Fips", by.y="combined_fips")
cleaned_facts = data.frame(joined_facts_results[c(-1, -2, -3, -length(joined_facts_results[1,]))])

scaled_facts = scale(cleaned_facts)


# Baseline category logit model without variable selection on entire dataset
library(VGAM)
samp <- sample(1:3100, 250)
logit <- vglm(joined_facts_results$Lean[samp] ~ ., data=data.frame(scaled_facts[samp,]), 
              family=(multinomial))
summary(logit)
plot(hclust(dist(scaled_facts[sample(length(scaled_facts[,1]), 100),]), method="complete"), main="Complete Linkage", labels=F)
plot(hclust(dist(scaled_facts[sample(length(scaled_facts[,1]), 100),]), method="single"), main="Single Linkage",  labels=F)
plot(hclust(dist(scaled_facts[sample(length(scaled_facts[,1]), 100),]), method="average"), main="Average Linkage",  labels=F)



#########PCA
prin_comp <- prcomp(scaled_facts, scale. = F)

summary(prin_comp)

#compute standard deviation of each principal component
std_dev <- prin_comp$sdev

#compute variance
pr_var <- std_dev^2

#check variance of first 10 components
pr_var[1:length(scaled_facts)]

#proportion of variance explained
prop_varex <- pr_var/sum(pr_var)
prop_varex[1:length(scaled_facts)]

#scree plot
plot(prop_varex, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")

#cumulative scree plot
plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")
abline(h=0.9)
abline(v=17)


# COMBINE THE FACTS WITH the LABEL COLUMN USING FIPS


autoplot(prin_comp, data = scaled_facts, colour = 'Lean', loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3) +
  ggtitle("First two PC's using Original Variables")


# Data from first 17 principal components
pc_data <- prin_comp$x[,1:17]

plot(hclust(dist(pc_data[sample(length(pc_data[,1]), 250),]), method="complete"), main="Complete Linkage", labels=F)

library(magrittr)
library(dplyr)
library(mclust)

c_pc = mclustBIC(pc_data, G=1:20)
plot(c_pc)

# Best cluster model is k=10 VVV
summary(c_pc)

em_clusters_pc = Mclust(pc_data, G=10, modelNames = "VVV")

# Possibly compare the EM results to kmeans with k=10

cluster_id_pc = em_clusters_pc$classification

cluster_count_pc = data.frame(cluster_id_pc) %>%
  group_by(cluster_id_pc) %>%
  summarise(n = n())

state_cluster_counts_pc = cbind(joined_facts_results[1:3], cluster_id_pc) %>% 
  group_by(state_abbreviation, cluster_id_pc) %>%
  summarise(count = n())

Lean_cluster_counts_pc = data.frame(cbind(joined_facts_results[length(joined_facts_results)], cluster_id_pc)) %>% 
  group_by(Lean, cluster_id_pc) %>%
  summarise(count = n())

View(Lean_cluster_counts_pc)

plot(jitter(cluster_id_pc), jitter(as.numeric(joined_facts_results$Lean)))

Lean_counts = joined_facts_results[length(joined_facts_results)] %>%
  group_by(Lean) %>%
  summarise(count = n())

merged_lean_counts_pc = merge(Lean_cluster_counts_pc, Lean_counts, by.x="Lean", by.y="Lean")
merged_lean_counts_pc$pct = merged_lean_counts_pc$count.x / merged_lean_counts_pc$count.y

plot(merged_lean_counts_pc$cluster_id, as.numeric(merged_lean_counts_pc$Lean), cex = 10 * merged_lean_counts_pc$pct, xlab = "Cluster ID", ylab = "Solid Dem to Solid GOP", main = "Percentage of Lean By Cluster Based on First 17 Pcs")


# Try clustering with scaled data instead of pc
c = mclustBIC(scaled_facts, G=5:15)
plot(c)

# Best cluster model is k=7 VEV
summary(c)

em_clusters = Mclust(scaled_facts, G=7, modelNames = "VEV")

# Possibly compare the EM results to kmeans with k=10

cluster_id = em_clusters$classification

cluster_count = data.frame(cluster_id) %>%
  group_by(cluster_id) %>%
  summarise(n = n())

state_cluster_counts = cbind(joined_facts_results[1:3], cluster_id) %>% 
  group_by(state_abbreviation, cluster_id) %>%
  summarise(count = n())

Lean_cluster_counts = data.frame(cbind(joined_facts_results[length(joined_facts_results)], cluster_id)) %>% 
  group_by(Lean, cluster_id) %>%
  summarise(count = n())

View(Lean_cluster_counts)

# Note that there are many more Solid GOP than Solid Dem counties
plot(jitter(cluster_id), jitter(as.numeric(joined_facts_results$Lean)))

merged_lean_counts = merge(Lean_cluster_counts, Lean_counts, by.x="Lean", by.y="Lean")
merged_lean_counts$pct = merged_lean_counts$count.x / merged_lean_counts$count.y

plot(merged_lean_counts$cluster_id, as.numeric(merged_lean_counts$Lean), cex = 10 * merged_lean_counts$pct, xlab = "Cluster ID", ylab = "Solid Dem to Solid GOP", main = "Percentage of Lean By Cluster")

data.frame(as.numeric(merged_lean_counts$Lean), merged_lean_counts$Lean)
```

